---
title: 'Kaggle Competiotion: Regression'
author: Lucas Calaff
jupyter: python3
format:
  html:
    toc: true
    code-fold: true
    theme: Cosmo
    embed-resources: true
execute:
  enabled: true
---


```{python}
import pandas as pd
import numpy as np
import plotnine as p9
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector, ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso,ElasticNet
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold
from sklearn.metrics import r2_score, classification_report, accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.metrics import  precision_score, recall_score, roc_auc_score, cohen_kappa_score
from sklearn.metrics._regression import mean_squared_error
from sklearn.feature_selection import SelectFromModel
from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor
from sklearn.inspection import permutation_importance
```

## Part Zero: Data set up and Exploration

```{python}
AME_train = pd.read_csv(r"C:\Users\ldcal\OneDrive\Desktop\Cal Poly Courses\Fall\GSB-S544\Coding Work\Pyython_Work\Kaggle_Comp\Kaggle_Regression\train_new.csv")
AME_test = pd.read_csv(r"C:\Users\ldcal\OneDrive\Desktop\Cal Poly Courses\Fall\GSB-S544\Coding Work\Pyython_Work\Kaggle_Comp\Kaggle_Regression\test_new.csv")
```

```{python}
# Looking at the string variables to see how many unique values they have

for x in ["Street", "Neighborhood", "Bldg Type", "House Style",
            "Roof Style", "Heating", "Electrical", "Functional", "Sale Type"]:

    print(AME_train[x].unique())
```

```{python}
(p9.ggplot(AME_train, p9.aes("Gr Liv Area", "SalePrice", fill = "Bldg Type"))
+p9.labs(title = "AME Housing Starting Chart", caption = "Training Data")
+p9.geom_point())
```

Looking through the dataset 

```{python}
AME_train.isna().sum().sort_values(ascending=False)
```

```{python}
AME_train.corr(numeric_only=True)
```

## Part One: Function and Pipeline Builders

```{python}
ct = ColumnTransformer(
    [("num", StandardScaler(), make_column_selector(dtype_include=np.number)),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output = False)
    , make_column_selector(dtype_exclude=np.number)),  
    ], 
    remainder = "drop" 

).set_output(transform = "pandas" ,)
```

```{python}
def Build_and_Test_Pipeline (pipeline_name,params,
                             model_name, model_sklearn, X, y):

    pipeline_name = Pipeline(
       [("preprocessing", ct),
        (model_name, model_sklearn)]
    )   

    grid = GridSearchCV(pipeline_name, params, cv = 5, scoring = "neg_root_mean_squared_error")
    fit = grid.fit(X, y)
    print(pd.DataFrame(fit.cv_results_)[["params", "mean_test_score"]])
    return (fit.best_params_)




def Final_Fit_and_RSME (pipeline_name, model_name, model_sklearn, X_training,
                         y_training, X_test, y_test, data):
    
    pipeline_name = Pipeline(
     [("preprocessing", ct),
        (model_name, model_sklearn)]
    )

    pipeline_name.fit(X_training, y_training)
    pred = pipeline_name.predict(X_test)
    RSME = np.sqrt(mean_squared_error(y_test, pred))

    data.append({"Model Name" : model_name, 
                        "RMSE" : RSME})
    return(RSME)
```

## Part Two: Training and Testing Models Using all the data

### 0. X and Y variable Setup

```{python}
AME_train_d_nan = AME_train.dropna()
AME_train_d_nan = AME_train_d_nan.drop(["PID"], axis = 1)

X1 = AME_train_d_nan.drop(columns=["SalePrice"])
y = AME_train_d_nan["SalePrice"]
y_log = np.log(y)

Part2_results = []
```

```{python}
X_train1, X_valid1, y_train_log1, y_valid_log1 = train_test_split(
    X1, y_log, test_size=0.25, random_state=42
)
```

### 1. Linear Regression

```{python}
LM_pipe = Pipeline(
       [("preprocessing", ct),
        ("LM", LinearRegression())]
)  

LM_pipe.fit(X_train1, y_train_log1)
LM_pred_valid = LM_pipe.predict(X_valid1)

LM_RMSE1 = np.sqrt(mean_squared_error(y_valid_log1, LM_pred_valid))

Part2_results.append({"Model Name" : "LM", 
                        "RMSE" : LM_RMSE1})
```

### 2. Ridge Regression

```{python}
alphas_tune = {"ridge__alpha": [1,2,3,4,5, 6,7]}

Build_and_Test_Pipeline("ridge_pipe", alphas_tune, "ridge", Ridge(), X_train1, y_train_log1)
```

```{python}
Final_Fit_and_RSME("Ridge_Final", "Ridge", Ridge(alpha = 5),
                        X_train1, y_train_log1, X_valid1, y_valid_log1, Part2_results)
```

### 3. Lasso Regression

```{python}
alphas_tune2 = {"Lasso__alpha": [0.0001, 0.0002, 0.00025 ,0.0003, 0.0004]}

Build_and_Test_Pipeline("Lasso_model", alphas_tune2,"Lasso", Lasso(), X_train1, y_train_log1)
```

```{python}
Final_Fit_and_RSME("Lasso_Final", "Lasso", Lasso(alpha = 0.00025),
                        X_train1, y_train_log1, X_valid1, y_valid_log1, Part2_results)
```

### 4. Elastic Net Regression

```{python}
param_grid_EN = {
    "EN__alpha":    [0.0003, 0.0006],
    "EN__l1_ratio": [0.5],
    "EN__max_iter": [2000, 5000, 10000],
}

Build_and_Test_Pipeline("EN_pipe", param_grid_EN, "EN", ElasticNet(), X_train1, y_train_log1)
```

```{python}
Final_Fit_and_RSME("EN_Final", "EN", ElasticNet(alpha = 0.00025, l1_ratio = 0.5, max_iter=2000),
                        X_train1, y_train_log1, X_valid1, y_valid_log1, Part2_results)
```

### 5. Tree Regression

```{python}
param_grid_tree = {
    "tree__max_depth": [None, 3, 5, 8, 12],
    "tree__min_samples_split": [2, 5, 10, 20],
    "tree__min_samples_leaf": [1, 2, 5, 10],
}

Build_and_Test_Pipeline("Tree_pipe", param_grid_tree, "tree", DecisionTreeRegressor(), X_train1, y_train_log1)
```

```{python}
Final_Fit_and_RSME("Tree_Final", "Tree", DecisionTreeRegressor(max_depth=8, min_samples_leaf=10, min_samples_split=10),
                        X_train1, y_train_log1, X_valid1, y_valid_log1, Part2_results)
```

### 6. KNN Regression 

```{python}
param_grid_knn = {
    "KNN__n_neighbors": range(1,10)}

Build_and_Test_Pipeline("KNN_pipe", param_grid_knn, "KNN", KNeighborsRegressor(), X_train1, y_train_log1)
```

```{python}
Final_Fit_and_RSME("Final_KNN", "KNN", KNeighborsRegressor(n_neighbors=8), X_train1, y_train_log1,
                        X_valid1, y_valid_log1, Part2_results)
```

### 7. Results 

```{python}
df2 = pd.DataFrame(Part2_results)
df2
```

For this part of Modeling we can see that the Elastic Net model perfromed the best of Regression using our Valid and Training data. Moving to our next models we will be doing this such as interaction varaibles and reducing the amout of x varaibles we work with to try and get better results. 

## Part Three: Reducing the X Varaibles Used 

### 0. Set up

```{python}
X2 = AME_train_d_nan.drop(["SalePrice", "Street", "House Style", "Overall Cond",
                             "Roof Style", "Yr Sold", "Sale Type"],
                            axis = 1)

Part3_results = []
```

```{python}
X_train2, X_valid2, y_train_log2, y_valid_log2 = train_test_split(
    X2, y_log, test_size=0.25, random_state=42
)
```

### 1. Linear Regression

```{python}
LM_pipe2 = Pipeline(
       [("preprocessing", ct),
        ("LM", LinearRegression())]
)  

LM_pipe2.fit(X_train2, y_train_log2)
LM_pred_valid2 = LM_pipe2.predict(X_valid2)

LM_RMSE2 = np.sqrt(mean_squared_error(y_valid_log2, LM_pred_valid2))

Part3_results.append({"Model Name" : "LM", 
                        "RMSE" : LM_RMSE2})
```

### 2. Ridge Regression

```{python}
param_Ridge_tune = {"ridge__alpha": [1,2,5, 10,20,50]}

Build_and_Test_Pipeline("Ridge_pipe2", param_Ridge_tune, "ridge",Ridge(), X_train2, y_train_log2)
```

```{python}
Final_Fit_and_RSME("Final_Ridge2", "Ridge", Ridge(alpha=2), X_train2, y_train_log2,
                    X_valid2, y_valid_log2, Part3_results)
```

### 3. Lasso Regression

```{python}
Param_Lasso_tune = {"Lasso__alpha": [0.00001,0.0001,0.00025, 0.0003]}
Build_and_Test_Pipeline("Lasso_Pipe2", Param_Lasso_tune, "Lasso", Lasso(), X_train2, y_train_log2)
```

```{python}
Final_Fit_and_RSME("Final_Lasso2", "Lasso", Lasso(alpha = 0.00025), X_train2, y_train_log2,
                    X_valid2, y_valid_log2, Part3_results)
```

### 4. Elastic Net Regression

```{python}
param_grid_EN2 = {
    "EN__alpha": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],   # strength of regularization
    "EN__l1_ratio": [0.1, 0.3, 0.5, 0.7, 0.9],           # 0 = ridge, 1 = lasso
    "EN__max_iter": [1000, 5000, 10000],                 # allow convergence
}

Build_and_Test_Pipeline("EN_Pipe", param_grid_EN2, "EN", ElasticNet(), X_train2, y_train_log2)
```

```{python}
Final_Fit_and_RSME("EN_Pipe2", "EN", ElasticNet(alpha = 0.0001, l1_ratio= 0.9, max_iter=1000), 
                    X_train2, y_train_log2, X_valid2, y_valid_log2, Part3_results)
```

### 5. Tree Regression

```{python}
param_grid_tree2 = {
    "tree__max_depth": [None, 3, 5, 8, 12],
    "tree__min_samples_split": [2, 5, 10, 20],
    "tree__min_samples_leaf": [1, 2, 5, 10],
}

Build_and_Test_Pipeline("Tree_pipe2", param_grid_tree2, "tree", DecisionTreeRegressor(),
                         X_train2, y_train_log2)
```

```{python}
Final_Fit_and_RSME("Tree_Final2", "Tree", DecisionTreeRegressor(max_depth = 8, min_samples_leaf =10, min_samples_split = 5 ),
                    X_train2, y_train_log2, X_valid2, y_valid_log2, Part3_results)
```

### 6. KNN Regression

```{python}
param_grid_knn2 = {
    "KNN__n_neighbors": [5,10,12,15,20,50]}

Build_and_Test_Pipeline("KNN_pipe2", param_grid_knn2, "KNN", KNeighborsRegressor(), X_train2, y_train_log2)
```

```{python}
Final_Fit_and_RSME("KNN_Pipe2", "KNN", KNeighborsRegressor(n_neighbors=10), 
                    X_train2, y_train_log2, X_valid2, y_valid_log2, Part3_results)
```

### 7. Results 

```{python}
df3 = pd.DataFrame(Part3_results)
df3
```

## Part Four: Polynomial Regression

### 0. Set Up

```{python}
# Selectors (you can also pass explicit lists of column names)
numeric_features = make_column_selector(dtype_include=["int64", "float64"])
categorical_features = make_column_selector(dtype_include=["object", "category"])

# Pipeline just for numeric part: scale -> poly
num_poly = Pipeline([
    ("scaler", StandardScaler()),
    ("poly", PolynomialFeatures(degree=2, include_bias=False))  # degree=2 to start
])

# ColumnTransformer: numeric gets num_poly, categoricals get one-hot
ct_poly = ColumnTransformer(
    transformers=[
        ("num", num_poly, numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ],
    remainder="drop"
)
```

### 1. Poly Pipeline

```{python}
PolyLM_pipe = Pipeline(
    [
        ("preprocessing", ct_poly),
        ("LM", LinearRegression())
    ]
)

param_grid_poly = {
    "preprocessing__num__poly__degree": [1, 2, 3,4,5],  # 1 = plain linear
}

grid_poly = GridSearchCV(
    PolyLM_pipe,
    param_grid=param_grid_poly,
    cv=5,
    scoring="neg_root_mean_squared_error"
)

grid_poly.fit(X_train1, y_train_log1)
print("Best params:", grid_poly.best_params_)
print("Best log-RMSE:", -grid_poly.best_score_)
```

```{python}
best_poly = grid_poly.best_estimator_
```

```{python}
poly_pred_valid = best_poly.predict(X_valid1)
poly_RMSE = np.sqrt(mean_squared_error(y_valid_log1, poly_pred_valid))
```

```{python}
part4_data = []

part4_data.append({"Model Name" : "Poly", 
                        "RMSE" : poly_RMSE})
df4 = pd.DataFrame(part4_data)
df4
```

## Part Five: Interaction Terms 

### 0. Set Up

```{python}
num_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("interact", PolynomialFeatures(
        degree=2,
        interaction_only=True,
        include_bias=False
    )),
])

ct_interact = ColumnTransformer(
    transformers=[
        ("num", num_pipe, numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ],
    remainder="drop"
)
```

```{python}
def Build_and_Test_inter_pipe (pipeline_name,params,
                             model_name, model_sklearn, X, y):

    pipeline_name = Pipeline(
       [("preprocessing", ct_interact),
        (model_name, model_sklearn)]
    )   

    grid = GridSearchCV(pipeline_name, params, cv = 5, scoring = "neg_root_mean_squared_error")
    fit = grid.fit(X, y)
    print(pd.DataFrame(fit.cv_results_)[["params", "mean_test_score"]])
    return (fit.best_params_)




def Final_Fit_Inter_and_RSME (pipeline_name, model_name, model_sklearn, X_training,
                         y_training, X_test, y_test, data):
    
    pipeline_name = Pipeline(
     [("preprocessing", ct_interact),
        (model_name, model_sklearn)]
    )

    pipeline_name.fit(X_training, y_training)
    pred = pipeline_name.predict(X_test)
    RSME = np.sqrt(mean_squared_error(y_test, pred))

    data.append({"Model Name" : model_name, 
                        "RMSE" : RSME})
    return(RSME)
```

```{python}
Part5_Results = []
```

### 1. Linear Model

```{python}
LM_Inter_Pipe = Pipeline(
    [("preprocessing", ct_interact), 
     ("LM", LinearRegression())]
)

LM_Inter_Pipe.fit(X_train1, y_train_log1)
LM_Inter_pred_valid = LM_Inter_Pipe.predict(X_valid1)

LM_Inter_RMSE = np.sqrt(mean_squared_error(y_valid_log1, LM_Inter_pred_valid))

Part5_Results.append({"Model Name" : "LM", 
                        "RMSE" : LM_Inter_RMSE})
```

### 2. Elastic Net

```{python}
param_Inter_EN = {
    "EN__alpha":    [0.0003, 0.00045, 0.0006, 0.0008, 0.001],
    "EN__l1_ratio": [ 0.5, 0.6, 0.7,0.8,0.9],
    "EN__max_iter": [5000, 10000],   
}
```

```{python}
Build_and_Test_inter_pipe("EN_Inter_Pipe", param_Inter_EN, "EN", ElasticNet(), X_train1, y_train_log1)
```

```{python}
Final_Fit_Inter_and_RSME("EN_Inter_Final", "EN", ElasticNet(alpha = 0.00045, l1_ratio = 0.9, max_iter=5000),
                        X_train1, y_train_log1, X_valid1, y_valid_log1, Part5_Results)
```

### 3. Results 

```{python}
df5 = pd.DataFrame(Part5_Results)
df5
```

## Part Six: Interpreting the Results 

```{python}
AllData = df2[["Model Name", "RMSE"]].copy()
AllData["Data_Type"] = "All"

Selected = df3[["Model Name", "RMSE"]].copy()
Selected["Data_Type"] = "Select"

Poly = df4[["Model Name", "RMSE"]].copy()
Poly["Data_Type"] = "Poly"

Inter = df5[["Model Name", "RMSE"]].copy()
Inter["Data_Type"] = "Inter"
```

```{python}
Regression_Models_Scores = pd.concat([AllData, Selected, Poly, Inter], ignore_index= True)

Regression_Models_Scores
```

Looking at the Table Above we can see that the EN Interaction Model Performed the best with a predicted valid RMSE of .1296, This value is just an estimate as we have no way in knowing until we interpret on the true data but this give us a sold understanding of how our models performed

## Part Seven: Prediction Extraction

```{python}
X_test = AME_test.drop("PID", axis = 1)
```

```{python}
Final_Pipeline = Pipeline(
    [("Preprocessing", ct_interact), 
     ("EN", ElasticNet(alpha = 0.00045, l1_ratio = 0.9, max_iter=5000))
     ]
    )

Final_Pipeline.fit(X_train1, y_train_log1)
```

```{python}
test_pred_log = Final_Pipeline.predict(X_test)
```

```{python}
test_pred = np.exp(test_pred_log)
```

```{python}
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.hist(AME_train["SalePrice"], bins=40)
plt.title("Train: Actual SalePrice")
plt.xlabel("SalePrice")

plt.subplot(1, 2, 2)
plt.hist(test_pred, bins=40)
plt.title("Test: Predicted SalePrice")
plt.xlabel("SalePrice")

plt.tight_layout()
plt.show()
```

```{python}
submission = pd.DataFrame({
    "PID": AME_test["PID"].values, 
    "SalePrice": test_pred
})

submission.to_csv("Lucas_Submission_1.csv", index = False)
```

